import collections
import pickle
import os

import pandas
import numpy as np
import tensorflow as tf

from setting import database_path, processed_data_path, non_embedding_variable, embedding_variable

class dataset_worker:

    def __init__(self, load_from_raw=False):
        self.dataset = pandas.read_csv(database_path, sep='|')
        self.dataset_loader(load_from_raw)

    def dataset_loader(self, load_from_raw):
        self.non_embedding_data, self.embedding_data = [], []
        if load_from_raw or (not os.path.isfile(processed_data_path[0])) or (not os.path.isfile(processed_data_path[1])):
            
            for row in range(self.dataset.shape[0]):
                new_non_embedding = []
                new_embedding = []
                for column_name in self.dataset.columns:
                    if column_name in non_embedding_variable:
                        new_non_embedding.append(float(self.dataset.at[row, column_name]))
                    elif column_name in embedding_variable:
                        new_embedding.append(float(self.dataset.at[row, column_name]))
                self.non_embedding_data.append(new_non_embedding.copy())
                self.embedding_data.append(new_embedding.copy())

            self.non_embedding_data = self.z_score(self.non_embedding_data)   

            self.embedding_data = self.z_score(self.embedding_data)
            
            with open(processed_data_path[0], 'wb') as fp:
                pickle.dump(self.non_embedding_data, fp)
            with open(processed_data_path[1], 'wb') as fp:
                pickle.dump(self.embedding_data, fp)

        else:
            with open(processed_data_path[0], 'rb') as fp:
                self.non_embedding_data = pickle.load(fp)
            with open(processed_data_path[1], 'rb') as fp:
                self.embedding_data = pickle.load(fp)

    def get_non_embedding_data(self):
        return self.non_embedding_data
    
    def get_embedding_data(self):
        return self.embedding_data
    
    def z_score(self, input):
        output = np.array(input)
        return (output - output.mean(axis=0, keepdims=True)) / output.std(axis=0, keepdims=True)
    
    def scaling(self, input):
        min_row = tf.reduce_min(input, axis=0)
        max_row = tf.reduce_max(input, axis=0)
        return (input - min_row) / (max_row - min_row)

    def dataset_analyser(self):
        self.dataset = pandas.read_csv(database_path, sep='|')
        table = collections.defaultdict(lambda: collections.defaultdict(int))
        catagory_lable = []

        for row in range(self.dataset.shape[0]):
            for columnName in self.dataset.columns:
                table[columnName][self.dataset.at[row, columnName]] += 1
        
        for column in table.keys():
            if len(table[column].keys()) < 100:
                catagory_lable.append(column)
                print(column, len(table[column].keys()))
        print(self.dataset.columns)
        print(self.dataset.shape[0])
        print(table['legitimate'])
